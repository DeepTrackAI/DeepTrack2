{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# !pip install deeptrack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1. Single-level trajectory analysis using Transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Imports the objects needed for this example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeptrack as dt\n",
    "from deeptrack.extras import datasets\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "import scipy.sparse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Overview\n",
    "\n",
    "In this example, [...]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining the dataset\n",
    "\n",
    "### 2.1 Defining the training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the STrajCh dataset\n",
    "datasets.load(\"STrajCh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_PATH = \"datasets/STrajCh/training/{file}.npz\"\n",
    "\n",
    "# read training data\n",
    "train_data = ()\n",
    "for file in (\"data\", \"indices\", \"labels\"):\n",
    "    train_data += (\n",
    "        scipy.sparse.load_npz(TRAINING_PATH.format(file=file)).toarray(),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitter(randset):\n",
    "    def inner(inputs):\n",
    "        data, indices, labels = inputs\n",
    "\n",
    "        # Convert to numpy array\n",
    "        data = data._value\n",
    "\n",
    "        # get indices of the rows belonging to randset\n",
    "        idx = np.where(indices == randset)[0]\n",
    "\n",
    "        sdata = data[idx][:, :2]\n",
    "        sdata = np.concatenate(\n",
    "            [\n",
    "                sdata,\n",
    "                np.array((0, *np.linalg.norm(np.diff(sdata, axis=0), axis=1)))[\n",
    "                    :, np.newaxis\n",
    "                ],\n",
    "                data[idx][:, 2:],\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        labels = labels[idx]\n",
    "\n",
    "        return sdata, labels\n",
    "\n",
    "    return inner\n",
    "\n",
    "\n",
    "nsamples = np.max(train_data[1])\n",
    "train_set = dt.Value(lambda: tuple(train_data)) >> dt.Lambda(\n",
    "    splitter, randset=lambda: np.random.randint(0, nsamples + 1)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.x Visualizing the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 3, figsize=((10, 10)), sharex=True, sharey=True)\n",
    "\n",
    "cmap = plt.cm.ScalarMappable(\n",
    "    norm=mpl.colors.Normalize(vmin=0.01, vmax=1.4), cmap=plt.cm.Oranges_r\n",
    ")\n",
    "\n",
    "for i in range(9):\n",
    "    data, labels = train_set.update()()\n",
    "\n",
    "    data = data[:, :2]\n",
    "\n",
    "    # extract changepoints\n",
    "    diff = np.array(labels[1:] - labels[:-1])\n",
    "    cp = (0, *np.where(diff != 0)[0] + 1, labels.shape[0])\n",
    "\n",
    "    for idxi, idxj in zip(cp[:-1], cp[1:]):\n",
    "        axs[i // 3, i % 3].plot(\n",
    "            data[idxi : idxj + 1, 0],\n",
    "            data[idxi : idxj + 1, 1],\n",
    "            c=cmap.to_rgba(labels[idxi])[0],\n",
    "            zorder=0,\n",
    "        )\n",
    "        axs[i // 3, i % 3].scatter(\n",
    "            data[idxi, 0], data[idxi, 1], c=\"g\", zorder=1, s=20\n",
    "        )\n",
    "\n",
    "    # set axis\n",
    "    axs[i // 3, i % 3].set_xlim([-0.6, 0.6])\n",
    "    axs[i // 3, i % 3].set_ylim([-0.6, 0.6])\n",
    "    axs[i // 3, i % 3].set_yticks([-0.5, 0, 0.5])\n",
    "    axs[i // 3, i % 3].set_xticks([-0.5, 0, 0.5])\n",
    "\n",
    "# set axis labels\n",
    "plt.setp(axs[:, 0], ylabel=\"y-centroid\")\n",
    "plt.setp(axs[-1, :], xlabel=\"x-centroid\")\n",
    "\n",
    "plt.subplots_adjust(wspace=0.05, hspace=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = train_set.update()()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.x Augment trajectories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AugmentTrajectories(rotate, translate, flip_x, flip_y):\n",
    "    \"\"\"\n",
    "    Returns a function that augments the input trajectories by applying\n",
    "    a random rotation, translation, and flip on the centroid coordinates.\n",
    "    \"\"\"\n",
    "\n",
    "    def inner(inputs):\n",
    "        data, labels = inputs\n",
    "\n",
    "        # Apply rotation and translation\n",
    "        centroids = data[:, :2]\n",
    "        centroids_x = (\n",
    "            centroids[:, 0] * np.cos(rotate)\n",
    "            + centroids[:, 1] * np.sin(rotate)\n",
    "            + translate[0]\n",
    "        )\n",
    "        centroids_y = (\n",
    "            centroids[:, 1] * np.cos(rotate)\n",
    "            - centroids[:, 0] * np.sin(rotate)\n",
    "            + translate[1]\n",
    "        )\n",
    "\n",
    "        # Apply flip\n",
    "        if flip_x:\n",
    "            centroids_x *= -1\n",
    "        if flip_y:\n",
    "            centroids_y *= -1\n",
    "\n",
    "        data[:, 0] = centroids_x\n",
    "        data[:, 1] = centroids_y\n",
    "\n",
    "        return data, labels\n",
    "\n",
    "    return inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_train_set = train_set >> dt.Lambda(\n",
    "    AugmentTrajectories,\n",
    "    rotate=lambda: np.random.rand() * 2 * np.pi,\n",
    "    translate=lambda: np.random.randn(2) * 0.05,\n",
    "    flip_x=lambda: np.random.randint(2),\n",
    "    flip_y=lambda: np.random.randint(2),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.x Pad trajectories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(pad_to):\n",
    "    def inner(inputs):\n",
    "        data, labels = inputs\n",
    "\n",
    "        tlen = int(np.shape(data)[0])\n",
    "\n",
    "        # create mask\n",
    "        indices = np.arange(tlen)\n",
    "        mask = np.stack(\n",
    "            [np.repeat(indices, tlen), np.tile(indices, tlen)], axis=1\n",
    "        )\n",
    "\n",
    "        # pad data\n",
    "        data = np.pad(data, ((0, pad_to - tlen), (0, 0)), mode=\"constant\")\n",
    "        labels = np.pad(labels, ((0, pad_to - tlen), (0, 0)), mode=\"constant\")\n",
    "\n",
    "        # pad mask\n",
    "\n",
    "        mask = np.pad(\n",
    "            mask,\n",
    "            ((0, pad_to ** 2 - np.shape(mask)[0]), (0, 0)),\n",
    "            mode=\"constant\",\n",
    "        )\n",
    "\n",
    "        return (data, mask), labels\n",
    "\n",
    "    return inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_to = np.unique(\n",
    "    train_data[1], return_counts=True\n",
    ")[1].max()\n",
    "\n",
    "padded_train_set = augmented_train_set >> dt.Lambda(pad, pad_to=pad_to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.x Defining data generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = dt.generators.ContinuousGenerator(\n",
    "    padded_train_set,\n",
    "    batch_size=8,\n",
    "    min_data_size=1024,\n",
    "    max_data_size=1025,\n",
    "    use_multi_inputs=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.x Defining the network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "model = dt.models.Transformer(\n",
    "    number_of_node_features=4,\n",
    "    dense_layer_dimensions=(32, 64, 96),\n",
    "    number_of_transformer_layers=3,\n",
    "    base_fwd_mlp_dimensions=256,\n",
    "    number_of_node_outputs=1,\n",
    "    node_output_activation=\"linear\",\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "\n",
    "class mae(tf.keras.losses.Loss):\n",
    "    def call(self, y_true, y_pred):\n",
    "        return tf.reduce_sum(tf.abs(y_true - y_pred)) / tf.math.count_nonzero(\n",
    "            y_true, dtype=tf.float32\n",
    "        )\n",
    "\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss=mae(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training the network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with generator:\n",
    "    model.fit(generator, epochs=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluating the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_PATH = \"datasets/STrajCh/validation/{file}.npz\"\n",
    "\n",
    "# read validation data\n",
    "val_data = ()\n",
    "for file in (\"data\", \"indices\", \"labels\"):\n",
    "    val_data += (\n",
    "        scipy.sparse.load_npz(VALIDATION_PATH.format(file=file)).toarray(),\n",
    "    )\n",
    "\n",
    "val_data, idxs , labels= val_data\n",
    "val_data = val_data[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample index\n",
    "idx = 100\n",
    "\n",
    "# get indices of the rows belonging to randset\n",
    "indices = np.where(idxs == idx)[0]\n",
    "\n",
    "val_sdata = val_data[indices][:, :2]\n",
    "val_sdata = np.concatenate(\n",
    "    [\n",
    "        val_sdata,\n",
    "        np.array((0, *np.linalg.norm(np.diff(val_sdata, axis=0), axis=1)))[\n",
    "            :, np.newaxis\n",
    "        ],\n",
    "        val_data[indices][:, 2:],\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "gt = labels[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# Compute predictions\n",
    "edges = np.array(\n",
    "    list(itertools.product(*(np.arange(val_sdata.shape[0]),) * 2))\n",
    ")\n",
    "pred = model([val_sdata[np.newaxis], edges[np.newaxis]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(gt[:, 0])\n",
    "plt.plot(pred.numpy()[0, :, 0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1a7f8d9ad56e90590fcee41b0180e1f5be02ee2520f1975e08f7f16dd529d162"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
