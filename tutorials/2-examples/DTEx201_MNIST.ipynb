{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "# Recognizing MNIST Digits\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/DeepTrackAI/DeepTrack2/blob/develop/tutorials/2-examples/DTEx201_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install deeplay deeptrack  # Uncomment if running on Colab/Kaggle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAL4blMUwBw0"
      },
      "source": [
        "This example trains a fully connected neural network to identify handwritten digits using MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import deeptrack as dt\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-DQVl_swBw0"
      },
      "source": [
        "## 1. Downloading the MNIST Dataset\n",
        "\n",
        "Start by downloading the MNIST dataset.\n",
        "\n",
        "The MNIST dataset consists of grayscale images of hand-written digits from 0 to 9. Each image is 28 pixels by 28 pixels. There're 60,000 training images and 10,000 test images.\n",
        "\n",
        "You will download it from the GitHub repository https://github.com/DeepTrackAI/MNIST_dataset, where the MNIST images are organized in two folders named `train` and `test`:\n",
        "\n",
        "> train/0_000000.png<br>\n",
        "> train/0_000001.png<br>\n",
        "> ...<br>\n",
        "> train/1_000000.png<br>\n",
        "> ...<br>\n",
        "\n",
        "> test/0_000000.png<br>\n",
        "> ...<br>\n",
        "> test/1_000000.png<br>\n",
        "> ...<br>\n",
        "\n",
        "The first digit in the filename is the label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if not os.path.isdir(\"MNIST_dataset\"):\n",
        "    os.system(\"git clone https://github.com/DeepTrackAI/MNIST_dataset\")\n",
        "\n",
        "train_path = os.path.join(\"MNIST_dataset\", \"mnist\", \"train\")\n",
        "test_path = os.path.join(\"MNIST_dataset\", \"mnist\", \"test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eel_k0ewBw4"
      },
      "source": [
        "## 2. Loading the Training Dataset\n",
        "\n",
        "The dataset is how you provide the network with training data. For this example, you will create the dataset by loading it from storage using the `LoadImage` class.\n",
        "\n",
        "You start by creating an `ImageFolder` object to hold the list of files corresponding to the training images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_files = dt.sources.ImageFolder(train_path)\n",
        "\n",
        "print(f\"Dataset contains {len(train_files)} train images\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, you create a pipeline that loads the normalized images and another one that loads the corresponding digit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pipeline to load the images.\n",
        "train_image = (\n",
        "    dt.LoadImage(path=train_files.path)\n",
        "    >> dt.NormalizeMinMax(0, 1)  # Normalize the image to [0, 1].\n",
        "    >> dt.MoveAxis(2, 0)  # Move the color channel to the first axis.\n",
        "    >> dt.pytorch.ToTensor(dtype=torch.float32) # Convert to a PyTorch tensor.\n",
        ")\n",
        "\n",
        "# Pipeline to load the labels.\n",
        "train_digit = (\n",
        "    dt.Value(train_files.label_name[0])  # Get the first digit of the name.\n",
        "    >> int  # Convert the digit to an integer.\n",
        "    >> np.array  # Convert to a NumPy array.\n",
        "    >> dt.pytorch.ToTensor(dtype=torch.long)  # Convert to a PyTorch tensor.\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, you cmbine them into a single dataset that returns a tuple with a MNIST image and the corresponding ground-truth digit value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset = dt.pytorch.Dataset(\n",
        "    train_image & train_digit,\n",
        "    train_files,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xBfGCnVwBxA"
      },
      "source": [
        "You can now plot some example images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(10, 6, figsize=(9, 18))\n",
        "\n",
        "for i, ax in enumerate(axs.flatten()):\n",
        "    image, digit = train_dataset[i * 1_000]\n",
        "    ax.imshow(image.numpy().squeeze(), cmap=\"gray\")\n",
        "    ax.set_title(digit)\n",
        "    ax.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1. Loading the Validation Dataset\n",
        "\n",
        "You follow similar steps to load the validation dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_files = dt.sources.ImageFolder(test_path)\n",
        "\n",
        "print(f\"Dataset contains {len(test_files)} test images\")\n",
        "\n",
        "test_image = (\n",
        "    dt.LoadImage(path=test_files.path)\n",
        "    >> dt.NormalizeMinMax(0, 1)\n",
        "    >> dt.MoveAxis(2, 0)\n",
        "    >> dt.pytorch.ToTensor(dtype=torch.float32)\n",
        ")\n",
        "test_digit = (\n",
        "    dt.Value(test_files.label_name[0])\n",
        "    >> int\n",
        "    >> np.array\n",
        "    >> dt.pytorch.ToTensor(dtype=torch.long)\n",
        ")\n",
        "\n",
        "test_dataset = dt.pytorch.Dataset(test_image & test_digit, test_files)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gI7KGmqZwBxF"
      },
      "source": [
        "## 2. Augmenting the Training Set\n",
        "\n",
        "In order to expand the dataset, you will augment it with affine and elastic augmentations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ey_phHMrwBxF"
      },
      "source": [
        "Affine augmentations consist of translating, rescaling, rotating and shearing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-04-01T16:23:45.768787Z",
          "iopub.status.busy": "2022-04-01T16:23:45.768287Z",
          "iopub.status.idle": "2022-04-01T16:23:45.771287Z",
          "shell.execute_reply": "2022-04-01T16:23:45.770787Z"
        },
        "id": "iXALwwXowBxG",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# How much to scale in x and y.\n",
        "scale = {\n",
        "    \"x\": lambda: 0.8 + np.random.rand() * 0.4,\n",
        "    \"y\": lambda: 0.8 + np.random.rand() * 0.4,\n",
        "}\n",
        "\n",
        "# How much to translate in x and y.\n",
        "translate_px = {\n",
        "    \"x\": lambda: int(np.random.randint(-2, 3)),\n",
        "    \"y\": lambda: int(np.random.randint(-2, 3)),\n",
        "}\n",
        "\n",
        "# Dummy property: whether to rotate or shear.\n",
        "should_rotate = lambda: np.random.randint(2)\n",
        "\n",
        "# If should rotate, how much.\n",
        "rotate = lambda should_rotate: (-0.35 + np.random.rand() * 0.7) * should_rotate\n",
        "\n",
        "# If not should rotate, how much shear.\n",
        "shear = lambda should_rotate: (\n",
        "    (-0.35 + np.random.rand() * 0.7) * (1 - should_rotate)\n",
        ")\n",
        "\n",
        "affine_transform = dt.Affine(\n",
        "    scale=scale,\n",
        "    translate_px=translate_px,\n",
        "    should_rotate=should_rotate,\n",
        "    shear=shear,   \n",
        "    order=2,\n",
        "    mode=\"constant\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9zf8NfJwBxI"
      },
      "source": [
        "Elastic augmentations distort the image elastically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-04-01T16:23:45.774288Z",
          "iopub.status.busy": "2022-04-01T16:23:45.773787Z",
          "iopub.status.idle": "2022-04-01T16:23:45.775815Z",
          "shell.execute_reply": "2022-04-01T16:23:45.776287Z"
        },
        "id": "Y2J2--NKwBxI"
      },
      "outputs": [],
      "source": [
        "# Amplitude of distortions.\n",
        "alpha = lambda: np.random.rand() * 60\n",
        "\n",
        "# Granularity of distortions.\n",
        "sigma = lambda: 5 + np.random.rand() * 2\n",
        "\n",
        "# Last dimension is not a channel, so it should be augmented too.\n",
        "ignore_last_dim = True\n",
        "\n",
        "elastic_transform = dt.ElasticTransformation(\n",
        "    alpha=alpha,\n",
        "    sigma=sigma,\n",
        "    ignore_last_dim=ignore_last_dim,\n",
        "    mode=\"constant\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbrKT7iGwBxK"
      },
      "source": [
        "Finally, since these distortions may cause pixels to fall outside the range of (0, 1), you need to clip the values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-04-01T16:23:45.779286Z",
          "iopub.status.busy": "2022-04-01T16:23:45.778786Z",
          "iopub.status.idle": "2022-04-01T16:23:45.781787Z",
          "shell.execute_reply": "2022-04-01T16:23:45.781286Z"
        },
        "id": "AKfRmo0PwBxK"
      },
      "outputs": [],
      "source": [
        "clip = dt.Clip(0, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzzalIsJwBxN"
      },
      "source": [
        "You now add the We add the augmentations to the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "augmentation = affine_transform >> elastic_transform >> clip\n",
        "\n",
        "augmented_train_image = (\n",
        "    dt.LoadImage(path=train_files.path)\n",
        "    >> dt.NormalizeMinMax(0, 1)\n",
        "    >> augmentation  # Apply the augmentation.\n",
        "    >> dt.MoveAxis(2, 0)\n",
        "    >> dt.pytorch.ToTensor(dtype=torch.float32)\n",
        ")\n",
        "\n",
        "augmented_train_dataset = dt.pytorch.Dataset(\n",
        "    augmented_train_image & train_digit,\n",
        "    train_files,\n",
        "    replace=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkOq0oYRwBxS"
      },
      "source": [
        "To ensure the data and the labels match up, plot some images print their correspoding label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for _ in range(10):\n",
        "    plt.figure(figsize=(9, 2))\n",
        "    \n",
        "    # Select the index of the image to augment.\n",
        "    i = np.random.randint(len(train_files))\n",
        "\n",
        "    # Get the original image and digit.\n",
        "    original_image, digit = train_dataset[i]\n",
        "\n",
        "    plt.subplot(1, 7, 1)\n",
        "    plt.imshow(original_image.numpy().squeeze(), cmap=\"gray\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"Original {digit}\")\n",
        "\n",
        "\n",
        "    for sub_plt in range(3, 8):\n",
        "\n",
        "        # Get the augmented image.\n",
        "        augmentation.update()\n",
        "        augmented_image, _ = augmented_train_dataset[i]\n",
        "\n",
        "        plt.subplot(1, 7, sub_plt)\n",
        "        plt.imshow(augmented_image.numpy().squeeze(), cmap=\"gray\")\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(\"Augmented\")\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZ_HouVVwBxU"
      },
      "source": [
        "## 3. Defining the Network\n",
        "\n",
        "The network used is a fully connected neural network.\n",
        "\n",
        "First, you define the network architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import deeptrack.deeplay as dl\n",
        "\n",
        "mlp_template = dl.MultiLayerPerceptron(\n",
        "    in_features=28 * 28,\n",
        "    hidden_features=[128, 128, 64, 64],\n",
        "    out_features=10,\n",
        ")\n",
        "for block in mlp_template.blocks:\n",
        "    block.append(dl.Layer(torch.nn.Dropout, 0.1))\n",
        "mlp_template[..., \"activation#-1\"].configure(torch.nn.Softmax, dim=-1)\n",
        "mlp = mlp_template.create()\n",
        "\n",
        "print(mlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, you compile the network, defining the loss function and the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "classifier_template = dl.Classifier(\n",
        "    model=mlp,\n",
        "    num_classes=10,\n",
        "    make_targets_one_hot=True,\n",
        "    optimizer=dl.RMSprop(lr=0.001)\n",
        ")\n",
        "classifier = classifier_template.create()\n",
        "    \n",
        "print(classifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8CAMVa9wBxX"
      },
      "source": [
        "## 4. Training the Network\n",
        "\n",
        "You can now train the network. First define the data loaders for the training and validation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    augmented_train_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "test_dataloader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=256,\n",
        "    shuffle=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, you can proceed with the training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = dl.Trainer(\n",
        "    max_epochs=100,\n",
        "    accelerator=\"auto\",\n",
        ")\n",
        "\n",
        "trainer.fit(\n",
        "    classifier, \n",
        "    train_dataloader,\n",
        "    test_dataloader,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can plot the history of the losses and metrics during the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = trainer.history.plot()\n",
        "axes[1].set_yscale(\"linear\")\n",
        "axes[1].set_ylim(0.0, 1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kD1JezplwBxZ"
      },
      "source": [
        "## 5. Evaluating the Training\n",
        "\n",
        "You can now proceed to evaluate the quality of the training. To do so, you need first to switch the `classifier` to evaluation mode (for example, to switch off the dropout layers)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "classifier.eval();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWxhPqa_wBxc"
      },
      "source": [
        "### 5.1. Comparing Ground-truth and Predicted Digits\n",
        "\n",
        "You can show a few images wirth the corresponding the true digit and the predicted digit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "execution": {
          "iopub.execute_input": "2022-04-01T16:23:46.484787Z",
          "iopub.status.busy": "2022-04-01T16:23:46.484287Z",
          "iopub.status.idle": "2022-04-01T16:23:46.497286Z",
          "shell.execute_reply": "2022-04-01T16:23:46.497787Z"
        },
        "id": "rOfnMGUswBxc",
        "outputId": "0b1f485c-9714-498c-fda7-09774cb8ac69"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(10, 6, figsize=(9, 20))\n",
        "\n",
        "for ax, (image, digit) in zip(axs.flatten(), test_dataset[::167]):\n",
        "    prediction_vector = classifier(image.unsqueeze(0))\n",
        "    prediction = prediction_vector.argmax().item()\n",
        "\n",
        "    ax.imshow(image.squeeze().numpy(), cmap=\"gray\")\n",
        "    ax.set_title(f\"Prediction: {prediction}\\nTrue: {digit.item()}\")\n",
        "    ax.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2. Calculating the Accuracy\n",
        "\n",
        "You can calculate the accuracy and error rate on the validation images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gt_digits = np.array([digit.numpy() for _, digit in test_dataset])\n",
        "predicted_digits = np.array([classifier(image.unsqueeze(0)).argmax().item()\n",
        "                             for image, _digit in test_dataset])\n",
        "\n",
        "accuracy = np.mean(gt_digits == predicted_digits)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Error rate:\", 1 - accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzLxsvD6wBxe"
      },
      "source": [
        "### 5.3 Visualizing Errors\n",
        "\n",
        "You can make a failure analysis by visualizing a few images that the model predicted incorrectly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "execution": {
          "iopub.execute_input": "2022-04-01T16:23:46.501288Z",
          "iopub.status.busy": "2022-04-01T16:23:46.500788Z",
          "iopub.status.idle": "2022-04-01T16:23:46.515785Z",
          "shell.execute_reply": "2022-04-01T16:23:46.515286Z"
        },
        "id": "kFGzbklCwBxe",
        "outputId": "c0bafc54-d3aa-47b7-9f5d-fd8f826fe9e9"
      },
      "outputs": [],
      "source": [
        "model_is_wrong = predicted_digits != gt_digits\n",
        "wrong_indices = np.where(model_is_wrong)[0]\n",
        "\n",
        "fig, axs = plt.subplots(10, 6, figsize=(9, 20))\n",
        "\n",
        "for ax in axs.flatten():\n",
        "    i = wrong_indices[np.random.randint(len(wrong_indices))]\n",
        "    image, digit = test_dataset[i]\n",
        "    prediction = classifier(image.unsqueeze(0)).argmax().item()\n",
        "    ax.imshow(image.squeeze().numpy(), cmap=\"gray\")\n",
        "    ax.set_title(f\"Prediction: {prediction}\\nTrue: {digit.item()}\")\n",
        "    ax.axis(\"off\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "1-MNIST.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "py_env_book",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
