{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file contains various functions needed that doesn't fit into the other files\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def Interpolate_gaps_in_plankton_positions(plankton_positions):\n",
    "    no_of_timesteps, no_of_cols = np.shape(plankton_positions)\n",
    "    for i in range(no_of_cols):\n",
    "        for j in range(1,no_of_timesteps-1):\n",
    "            if np.isnan(plankton_positions[j,i]) and np.any(np.isnan([plankton_positions[j-1,i], plankton_positions[j+1,i]]))==False:\n",
    "                plankton_positions[j,i] = (plankton_positions[j-1,i] + plankton_positions[j+1,i])/2\n",
    "    return plankton_positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import label\n",
    "def get_blob_center(label,array):\n",
    "    x, y = np.where(array==label)\n",
    "    x_center = np.sum(x)/len(x)\n",
    "    y_center = np.sum(y)/len(y)\n",
    "    \n",
    "    return x_center, y_center\n",
    "\n",
    "\n",
    "def get_blob_centers(prediction):\n",
    "    prediction[prediction<0.7]=0\n",
    "    labeled_array, num_features = label(prediction, structure = [[1,1,1],\n",
    "                                                                 [1,1,1],\n",
    "                                                                 [1,1,1]])\n",
    "    centers = get_blob_center(1, labeled_array)\n",
    "    for i in range(2,num_features):\n",
    "        if np.count_nonzero(labeled_array==(i)) > 0:\n",
    "            centers = np.vstack((centers,get_blob_center(i, labeled_array)))\n",
    "    return centers\n",
    "\n",
    "def Extract_positions_from_prediction(im_stack, model, layer):\n",
    "    prediction = model.predict(im_stack)[0, :, :, layer]\n",
    "    positions = get_blob_centers(prediction)\n",
    "    return positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RunningAverage(folder_path, tot_no_of_frames, center_frame):\n",
    "    import os  \n",
    "    from deeptrack.features import LoadImage, Feature\n",
    "    import numpy as np\n",
    "    list_paths = os.listdir(folder_path)\n",
    "    first_file_format = list_paths[0][-3:]\n",
    "    for i in range(len(list_paths)):\n",
    "        if list_paths[i][-3:] != first_file_format:\n",
    "            print('Only the images to be analyzed can be in the folder with the images.')\n",
    "            break\n",
    "    frames_one_dir = int(tot_no_of_frames/2)\n",
    "    first_image = np.asarray(LoadImage(folder_path +'\\\\' + list_paths[0]).resolve())\n",
    "    mean_image = np.zeros(first_image.shape)\n",
    "    start_point, end_point = max(center_frame - frames_one_dir,0), min(center_frame + frames_one_dir + 1, len(list_paths))\n",
    "    for i in range(start_point, end_point):\n",
    "        mean_image += np.asarray(LoadImage(folder_path +'\\\\' + list_paths[max(i,0)]).resolve()) / tot_no_of_frames\n",
    "    \n",
    "    return mean_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalize_image(image, min_value=0, max_value=1):\n",
    "    min_im = np.min(image)\n",
    "    max_im = np.max(image)\n",
    "    image = image/(max_im-min_im) * (max_value - min_value)\n",
    "    return image - np.min(image) + min_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "class Plankton:\n",
    "    def __init__(self, position, number_of_timesteps, current_timestep):\n",
    "        self.positions = np.zeros(shape = (number_of_timesteps, 2), dtype='object')*np.nan\n",
    "        self.positions[current_timestep,:] = position\n",
    "        self.number_of_timesteps = number_of_timesteps\n",
    "    \n",
    "    def add_position(self, position, timestep):\n",
    "        self.positions[timestep,:] = position\n",
    "         \n",
    "    def get_latest_position(self, timestep, threshold):\n",
    "        latest_position = [np.nan, np.nan]\n",
    "        for i in range(self.number_of_timesteps - timestep, min(threshold + self.number_of_timesteps - timestep, self.number_of_timesteps+1)):\n",
    "            if np.isfinite(self.positions[-i,0]):\n",
    "                latest_position = self.positions[-i,:]\n",
    "                break\n",
    "        return latest_position\n",
    "    \n",
    "            \n",
    "    def get_mean_velocity(self):\n",
    "        no_of_timesteps = len(self.positions[:,0])\n",
    "        no_of_numbers = sum(x is not np.nan for x in self.positions[:,0].tolist())\n",
    "        mean_velocity = 0\n",
    "        if no_of_numbers > 1:\n",
    "            for i in range(no_of_timesteps-1):\n",
    "                mean_velocity += np.nansum(np.linalg.norm(self.positions[i,:]-self.positions[i+1,:]))/(no_of_numbers)\n",
    "        else:\n",
    "            mean_velocity = 0\n",
    "        return mean_velocity\n",
    "        \n",
    "def Initialize_plankton(positions, number_of_timesteps):\n",
    "    no_of_plankton = np.shape(positions)[0]\n",
    "    list_of_plankton = []\n",
    "    \n",
    "    for i in range(no_of_plankton):\n",
    "        globals()['plankton%d' % i] = Plankton(positions[i,:], number_of_timesteps, 0)\n",
    "        list_of_plankton.append(globals()['plankton%d' % i])\n",
    "    return list_of_plankton\n",
    "\n",
    "def Update_list_of_plankton(list_of_plankton, positions, max_dist, timestep, threshold, extrapolate):\n",
    "    if np.any(np.isnan(positions)):\n",
    "        return list_of_plankton\n",
    "    \n",
    "    no_of_plankton = len(list_of_plankton)\n",
    "    no_of_positions = len(positions)\n",
    "    plankton_positions = np.zeros([no_of_plankton, 2])\n",
    "    \n",
    "    for i in range(no_of_plankton):\n",
    "        plankton_positions[i,:] = list_of_plankton[i].get_latest_position(timestep = timestep, threshold = threshold)\n",
    "\n",
    "    \n",
    "    if extrapolate == True and timestep > 1:\n",
    "        plankton_positions = Extrapolate_positions(plankton_positions)\n",
    "        \n",
    "    if len(positions.shape)==1:\n",
    "        positions = np.reshape(positions, (-1, 2))\n",
    "        no_of_positions = len(positions)\n",
    "    \n",
    "    distances = cdist(positions, plankton_positions)\n",
    "    \n",
    "    for i in range(no_of_positions):\n",
    "        if np.nanmin(distances[i,:]) > max_dist:\n",
    "            position = positions[i,:]\n",
    "            globals()['plankton%d' % (len(list_of_plankton))] = Plankton(position, plankton0.number_of_timesteps, timestep)\n",
    "            list_of_plankton.append(globals()['plankton%d' % (len(list_of_plankton))])\n",
    "        else:\n",
    "            if np.sum(distances[i,:] < max_dist) > 1:\n",
    "                temp_indices = np.where(distances[i,:] < max_dist)[0]\n",
    "                temp_dists = distances[i,:][temp_indices]\n",
    "                temp_veldiffs = np.zeros((len(temp_indices),1))\n",
    "                for j in range(len(temp_dists)):\n",
    "                    temp_veldiffs[j] = np.abs(temp_dists[j]-list_of_plankton[temp_indices[j]].get_mean_velocity())\n",
    "                temp_min_vel = np.min(temp_veldiffs)\n",
    "                temp_min_index = np.where(temp_veldiffs==temp_min_vel)[0][0]\n",
    "                list_of_plankton[temp_indices[temp_min_index]].add_position(positions[i,:], timestep)\n",
    "                \n",
    "            else:\n",
    "                temp_min_dist = np.nanmin(distances[i,:])\n",
    "                temp_min_index = np.nonzero(distances[i,:]==temp_min_dist)[0][0]\n",
    "                list_of_plankton[temp_min_index].add_position(positions[i,:], timestep)\n",
    "    return list_of_plankton\n",
    "\n",
    "def Interpolate_gaps_in_plankton_positions(plankton_positions):\n",
    "    no_of_timesteps, no_of_cols = np.shape(plankton_positions)\n",
    "    for i in range(no_of_cols):\n",
    "        for j in range(1,no_of_timesteps-1):\n",
    "            if np.isnan(plankton_positions[j,i]) and np.any(np.isnan([plankton_positions[j-1,i], plankton_positions[j+1,i]]))==False:\n",
    "                plankton_positions[j,i] = (plankton_positions[j-1,i] + plankton_positions[j+1,i])/2\n",
    "    return plankton_positions\n",
    "\n",
    "def Extrapolate_positions(plankton_positions):\n",
    "    no_of_timesteps, no_of_cols = np.shape(plankton_positions)\n",
    "    for i in range(no_of_cols):\n",
    "        for j in range(1,no_of_timesteps):\n",
    "            if np.isnan(plankton_positions[j,i]) and np.any(np.isnan([plankton_positions[j-1,i], plankton_positions[j-2,i]]))==False:\n",
    "                plankton_positions[j,i] = (2 * plankton_positions[j-1,i] - plankton_positions[j-2,i])\n",
    "    return plankton_positions\n",
    "\n",
    "\n",
    "\n",
    "def Trim_positions_from_stationary_planktons(positions, min_distance):\n",
    "    no_of_timesteps, no_of_plankton = positions[:,0::2].shape\n",
    "    cols_to_delete = []\n",
    "    \n",
    "    for j in range(no_of_plankton):\n",
    "        temp_dist=0\n",
    "        temp_positions = positions[:,2*j:2*j+1]\n",
    "        \n",
    "        for i in range(len(temp_positions[:,0])-1):\n",
    "            temp_dist += np.nansum(np.linalg.norm(temp_positions[i,:]-temp_positions[i+1,:]))\n",
    "        temp_dist = np.nansum(temp_dist)\n",
    "        \n",
    "        if temp_dist < min_distance:\n",
    "            cols_to_delete.append(2*j)\n",
    "            cols_to_delete.append(2*j+1)\n",
    "        \n",
    "    new_positions = np.delete(positions,cols_to_delete,1)\n",
    "    return new_positions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "def Make_video(path_frames, save_path, fps, no_of_frames):\n",
    "    # Make sure that the path_frames name looks like ...\\\\frame%d.jpg (if the frames\n",
    "    # are named frame0, frame1, frame2...) or ...\\\\frame%3d.jpg (if the frames are named frame001, frame002...) \n",
    "    # so that the counter works.\n",
    "    img_array = []\n",
    "    for i in range(no_of_frames):\n",
    "    \n",
    "        img = cv2.imread(path_frames % i)\n",
    "\n",
    "        height, width, layers = img.shape\n",
    "        size = (width,height)\n",
    "        img_array.append(img)\n",
    "        print(i)\n",
    "\n",
    "    out = cv2.VideoWriter(save_path ,cv2.VideoWriter_fourcc(*'DIVX'), fps, size)\n",
    "\n",
    "    for i in range(len(img_array)):\n",
    "        out.write(img_array[i])\n",
    "    out.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
